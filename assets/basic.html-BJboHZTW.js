import{_ as s,c as i,b as e,o as n}from"./app-Bmvlbfib.js";const l={};function r(a,t){return n(),i("div",null,t[0]||(t[0]=[e(`<h2 id="redis数据类型" tabindex="-1"><a class="header-anchor" href="#redis数据类型"><span>Redis数据类型</span></a></h2><p><strong>核心数据类型</strong> (5种)</p><ol><li><strong>String (字符串)</strong>： 最简单的类型，可存文本、整数或浮点数，二进制安全。支持自增/自减操作。</li><li><strong>Hash (哈希)</strong>： 字段-值对的集合，非常适合存储对象。</li><li><strong>List (列表)</strong>： 按插入顺序排序的字符串元素列表，基于双向链表实现，可用作栈或队列。</li><li><strong>Set (集合)</strong>： 无序的、元素唯一的字符串集合，支持交集、并集、差集等运算。</li><li><strong>Sorted Set (有序集合)</strong>： 类似 Set，但每个元素关联一个分数(score)，按分数排序，元素唯一，适用于排行榜。</li></ol><hr><p><strong>扩展数据类型</strong> (基于核心类型实现)</p><ol><li><strong>Bitmaps (位图)</strong>： 本质是 String，提供位级别的操作（如设置、统计）。极其节省空间，适用于二值状态统计（如用户签到）。</li><li><strong>HyperLogLogs (基数统计)</strong>： 本质是 String，用于估计一个集合中不重复元素的基数（数量）。提供近似去重计数，标准误差 &lt;1%，极其节省内存。</li><li><strong>Streams (流)</strong>： 本质是 List 的增强版，提供了一个完整的、支持多消费者的消息队列功能。支持消息持久化、消费者组、消息确认机制。</li><li><strong>Geo(地理坐标)</strong>：Redis3.2 推出的，地理位置定位，用于存储地理位置信息，并对存储的信息进行操作。</li></ol><div class="language-c line-numbers-mode" data-highlighter="shiki" data-ext="c" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">/* Redis 对象结构体定义 */</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">typedef</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;"> struct</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> redisObject {</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    // 对象类型（4位），如字符串、列表、哈希等</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    unsigned</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> type:</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">4</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">;    </span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    </span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    // 编码方式（4位），标识底层数据结构实现，如int、embstr、raw等</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    unsigned</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> encoding:</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">4</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">;    </span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    </span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    // LRU/LFU 信息（24位）:</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    // - 若配置LRU策略：记录以秒为单位的最近一次访问时间</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    // - 若配置LFU策略：高16位记录分钟级访问时间，低8位记录逻辑访问次数</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    unsigned</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> lru:LRU_BITS;    </span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    </span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    // 引用计数，用于内存回收（计数为0时对象可被回收）</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    int</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> refcount;            </span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    </span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    // 指向实际存储数据的指针</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    void</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;"> *</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">ptr;                </span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">} robj;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>LFU（最不经常使用）计数机制</strong></p><p>LFU 的访问次数称为<strong>逻辑访问次数</strong>，其更新规则如下：</p><ol><li><strong>生成随机数</strong>：生成一个 <code>0~1</code> 之间的随机数 <code>R</code>。</li><li><strong>计算概率值</strong>：计算 <code>1/(旧次数 * lfu_log_factor + 1)</code>，记为 <code>P</code>（<code>lfu_log_factor</code> 默认为 10）。</li><li><strong>计数递增</strong>：如果 <code>R &lt; P</code>，则计数器 +1（最大不超过 255）。</li><li><strong>计数衰减</strong>：访问次数会随时间衰减。距离上一次访问时间每间隔 <code>lfu_decay_time</code> 分钟（默认 1），计数器减 1。</li></ol><hr><p><strong>核心要点总结</strong></p><ul><li><strong>RedisObject</strong> 是 Redis 数据存储的底层通用结构。</li><li><strong><code>lru</code> 字段</strong>具有双重含义，取决于 Redis 使用的淘汰策略： <ul><li>配置为 LRU 时，记录以秒为单位的最近访问时间。</li><li>配置为 LFU 时，高16位记录最近访问时间（分钟级），低8位存储逻辑访问次数。</li></ul></li><li><strong>LFU 的计数</strong>是一个概率性的近似值，而非精确计数，通过随机数和概率公式控制增长，并会随时间自动衰减，从而更精准地识别热点数据。</li></ul><p><strong>为什么String类型用SDS实现，而不是C字符串？</strong><br> SDS (Simple Dynamic String) 相比C原生字符串有巨大优势：</p><ul><li><strong>O(1)获取长度</strong>：SDS有<code>len</code>属性，直接获取字符串长度，而C字符串需要遍历。</li><li><strong>避免缓冲区溢出</strong>：SDS API会先检查空间是否足够，不足则自动扩展。</li><li><strong>二进制安全</strong>：SDS可以存储任意二进制数据，包括<code>\\0</code>，因为它靠<code>len</code>判断结束，而不是空字符。</li><li><strong>内存预分配</strong>：减少修改字符串时带来的内存重分配次数。</li></ul><p><strong>ZSet的底层实现是什么？为什么用跳跃表？</strong></p><ul><li><strong>底层实现</strong>：<strong>跳跃表 (skiplist) + 哈希表 (dict)</strong> 的混合结构。</li><li><strong>为什么用跳跃表</strong>： <ol><li><strong>支持范围查询</strong>：跳跃表基于排序的链表结构，可以高效地实现 <code>ZRANGE</code>, <code>ZRANK</code> 等范围操作，效率远高于平衡树。</li><li><strong>实现相对简单</strong>：比平衡树（如AVL、红黑树）更容易实现和调试。</li><li><strong>平均性能好</strong>：插入、删除、查找的平均时间复杂度都是 <strong>O(log N)</strong>，性能与平衡树相当。</li></ol></li></ul><h2 id="redis-高性能原因" tabindex="-1"><a class="header-anchor" href="#redis-高性能原因"><span>Redis 高性能原因</span></a></h2><ol><li><strong>基于内存</strong>： 数据主要存储在内存中，读写操作直接操作内存，避免了磁盘 I/O 瓶颈。</li><li><strong>单线程架构</strong>： <ul><li>避免了多线程的上下文切换和竞争开销。</li><li>保证了原子操作的线程安全，简化了数据结构的实现。</li></ul></li><li><strong>高效的数据结构</strong>： <ul><li>设计了多种精心优化的底层数据结构（如 SDS、跳跃表、压缩列表等），最大限度利用内存并减少操作时间。</li></ul></li><li><strong>I/O 多路复用</strong>： <ul><li>使用 <code>epoll</code> 等机制处理大量连接，保证在单线程下也能高效进行网络 I/O。</li></ul></li><li><strong>其他优化</strong>： <ul><li><strong>虚拟内存机制</strong>： 应对物理内存不足。</li><li><strong>渐进式 rehash</strong>： 在扩容时避免长时间阻塞。</li></ul></li></ol><p><strong>总结</strong>： Redis为基于内存的非关系型数据库，故瓶颈不在CPU，而是内存和网络IO。<strong>内存访问 + 单线程避免竞争 + I/O 多路复用 + 优化数据结构</strong> 共同造就了其极致性能。</p><h2 id="redis-持久化机制" tabindex="-1"><a class="header-anchor" href="#redis-持久化机制"><span>Redis 持久化机制</span></a></h2><p>Redis 提供两种核心持久化机制：<strong>RDB</strong> 和 <strong>AOF</strong>，可单独或组合使用，确保数据在重启后不丢失。</p><h3 id="rdb-redis-database" tabindex="-1"><a class="header-anchor" href="#rdb-redis-database"><span>RDB（Redis Database）</span></a></h3><p>RDB全称（Redis数据备份文件），也被叫做Redis数据快照。简单来说就是在指定时间间隔内，将内存中的数据集生成一个<strong>全量快照</strong>，存储到二进制压缩文件（默认 <code>dump.rdb</code>）。</p><p><strong>执行时机</strong>：</p><ul><li><p><strong>手动触发</strong>：<code>SAVE</code>（同步阻塞）或 <code>BGSAVE</code>（后台异步）。</p></li><li><p><strong>自动触发</strong>：通过redis.conf配置规则（如 <code>save 900 1</code> 表示 900 秒内至少 1 个键变更则触发 <code>BGSAVE</code>）。</p></li><li><p><strong>关机时</strong>：如果开启了RDB，正常关闭服务时会自动执行一次<code>SAVE</code>。</p></li></ul><p><strong>RDB bgsave 基本流程</strong></p><ol><li><strong>Fork子进程</strong>：主进程创建一个子进程，仅 <code>fork</code> 时有短暂阻塞。</li><li><strong>子进程写文件</strong>：子进程把当前内存中的数据<strong>复制</strong>并写入到一个<strong>新的临时RDB文件</strong>中。</li><li><strong>替换旧文件</strong>：写完後，用新文件<strong>替换</strong>掉旧的RDB文件。</li></ol><p>基于 <strong>fork + copy-on-write</strong>。父进程继续服务，子进程遍历内存数据写入 RDB 文件。利用写时复制技术保证子进程数据是 <code>fork</code> 时刻的快照。</p><p><strong>优点</strong>：</p><ol><li><strong>性能最大化</strong>：<code>bgsave</code> 方式由子进程负责持久化，主进程无需进行任何磁盘 I/O 操作，保证了 Redis 的高性能。</li><li><strong>恢复速度快</strong>：相对于 AOF，RDB 是紧凑的二进制压缩文件。在重启恢复大数据集时，速度比 AOF 快很多。</li><li><strong>适合灾难备份</strong>：RDB 文件是一个单一的紧凑文件，非常适合用于备份、全量复制和灾难恢复。例如，可以把每小时的一个 RDB 文件存档。</li><li><strong>数据文件更小</strong>：在恢复时，RDB 文件比 AOF 文件体积更小。</li></ol><p><strong>缺点</strong>：</p><ol><li><strong>数据安全性低</strong>：因为每隔一段时间才备份一次，如果中途宕机，从上一次备份到宕机时的数据会丢失。（这是最主要的缺点）</li><li><strong>fork 操作的潜在风险</strong>：虽然 <code>bgsave</code> 是后台操作，但 fork 子进程的过程可能会阻塞主进程（尽管通常很短）。如果数据量巨大，fork 耗时较长，且内存数据越大，阻塞时间越长。</li></ol><blockquote><p><strong>save 60 1000 代表什么？</strong></p><p>代表：在 <strong>60秒内</strong>，如果发生了至少 <strong>1000次数据修改</strong>，则自动触发一次 <code>BGSAVE</code>。</p></blockquote><h3 id="aof-append-only-file" tabindex="-1"><a class="header-anchor" href="#aof-append-only-file"><span>AOF（Append Only File）</span></a></h3><p>AOF全称为Append Only File（追加文件）。以命令日志文件的形式，记录Redis处理的每一个写命令，重启服务时重新执行这些命令来恢复数据。如果两个都配了优先加载AOF。</p><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">#指定 AOF 持久化文件的文件名</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">appendfilename</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;appendonly.aof&quot;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>执行流程</strong></p><ol><li><strong>命令追加</strong>：写命令执行后追加到 AOF 缓冲区。</li><li><strong>文件写入</strong>：根据 <code>appendfsync</code> 策略将缓冲区内容写入内核页面缓存。</li><li><strong>文件同步</strong>：根据策略将内核缓存数据同步（<code>fsync</code>）到磁盘。</li></ol><p><strong>执行时机，appendfsync 策略</strong></p><ul><li><strong>always</strong>：每个写命令同步到磁盘。最安全，性能最低。</li><li><strong>everysec（默认）</strong>：每秒同步一次。均衡策略，最多丢失 1 秒数据。</li><li><strong>no</strong>：由操作系统决定同步时机。性能最好，数据最不安全。</li></ul><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"> #表示每执行一次写命令，立即记录到AOF文件 </span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">appendfsync</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> always</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">#写命令执行完先放入AOF缓冲区，然后表示每隔1秒将缓冲区数据写到AOF文件，是默认方案。</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">appendfsync</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> everysec</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> </span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">#写命令执行完先放入AOF缓冲区，由操作系统决定何时将缓冲区内容写回磁盘 </span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">appendfsync</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> no</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>优点</strong></p><ol><li><strong>数据安全性高</strong>：根据策略最多丢失 1 秒数据（甚至不丢失）。</li><li><strong>可读性强</strong>：文本协议格式，可手动查看（不建议修改）。</li></ol><p><strong>缺点</strong></p><ol><li><strong>文件体积大</strong>：通常比同数据 RDB 文件大。</li><li><strong>恢复速度慢</strong>：需逐条执行命令，比 RDB 慢。</li><li><strong>性能影响较大</strong>：即使使用 <code>everysec</code>，性能仍略低于 RDB。</li></ol><h4 id="aof-重写-rewrite" tabindex="-1"><a class="header-anchor" href="#aof-重写-rewrite"><span>AOF 重写（Rewrite）</span></a></h4><p>因为是记录命令，AOF文件会比RDB文件大的多。而且AOF会记录对同一个key的多次写操作，但只有最后一次写操作才有意义。通过执行bgrewriteaof命令，可以让AOF文件执行重写功能，用最少的命令达到相同效果。<strong>目的</strong>：解决 AOF 文件膨胀问题，生成最小命令集合的新文件。</p><p><strong>重写基本流程</strong></p><ol><li><p><strong>Fork 子进程</strong>：主进程创建一个子进程。</p></li><li><p><strong>子进程写新文件</strong>：子进程<strong>读取当前数据库的数据</strong>，并为每个键值对生成一条<strong>最新的、最简洁的</strong>写入命令，写入到一个<strong>临时的新 AOF 文件</strong>中。</p><ul><li><em>注意：子进程是基于当前数据快照来生成命令的，而不是分析旧的 AOF 文件。</em></li></ul></li><li><p><strong>追加新命令</strong>：在子进程重写期间，所有新的写命令不仅会追加到<strong>原来的 AOF 缓冲区</strong>，也会追加到一个<strong>重写缓冲区</strong>。</p></li><li><p><strong>替换旧文件</strong>：子进程完成新文件的写入后，通知主进程。主进程将<strong>重写缓冲区</strong>中的命令追加到新的 AOF 文件中，然后<strong>原子性地替换</strong>掉旧的 AOF 文件。</p></li></ol><ul><li><p><strong>触发方式</strong>：手动（<code>BGREWRITEAOF</code>）或自动（配置 <code>auto-aof-rewrite-percentage</code> 和 <code>auto-aof-rewrite-min-size</code>）。</p></li><li><p><strong>实现</strong>：<code>fork</code> 子进程完成，不影响主进程。新写命令同时写入旧 AOF 缓冲和重写缓冲，确保数据一致性。</p></li></ul><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">#AOF文件比上次文件增长超过多少百分比则触发重写</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">auto-aof-rewrite-percentage</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 100</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">#AOF文件体积最小多大以上才触发重写</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">auto-aof-rewrite-min-size</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> 64mb</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="rdb-与-aof-对比" tabindex="-1"><a class="header-anchor" href="#rdb-与-aof-对比"><span>RDB 与 AOF 对比</span></a></h4><table><thead><tr><th style="text-align:left;">特性</th><th style="text-align:left;">RDB (快照模式)</th><th style="text-align:left;">AOF (日志模式)</th></tr></thead><tbody><tr><td style="text-align:left;"><strong>持久化原理</strong></td><td style="text-align:left;">定时对内存做<strong>全量</strong>快照</td><td style="text-align:left;">记录每一次<strong>写命令</strong></td></tr><tr><td style="text-align:left;"><strong>数据一致性</strong></td><td style="text-align:left;"><strong>弱</strong>，可能丢失最后一次快照后的所有数据</td><td style="text-align:left;"><strong>强</strong>，最多丢失一秒的数据（默认配置）</td></tr><tr><td style="text-align:left;"><strong>文件大小</strong></td><td style="text-align:left;"><strong>小</strong>（二进制压缩格式）</td><td style="text-align:left;"><strong>大</strong>（日志文本格式，即使重写后也可能比RDB大）</td></tr><tr><td style="text-align:left;"><strong>恢复速度</strong></td><td style="text-align:left;"><strong>快</strong>（直接加载数据到内存）</td><td style="text-align:left;"><strong>慢</strong>（需要逐条执行命令）</td></tr><tr><td style="text-align:left;"><strong>性能影响</strong></td><td style="text-align:left;"><strong>fork子进程时内存开销大</strong>，可能阻塞</td><td style="text-align:left;"><strong>持续写入开销</strong>，但重写时也会有fork开销</td></tr><tr><td style="text-align:left;"><strong>优先级</strong></td><td style="text-align:left;">低</td><td style="text-align:left;"><strong>高</strong>（如果同时开启，Redis重启优先使用AOF恢复）</td></tr><tr><td style="text-align:left;"><strong>适用场景</strong></td><td style="text-align:left;">追求<strong>快速重启</strong>、允许<strong>分钟级数据丢失</strong></td><td style="text-align:left;">要求<strong>数据安全</strong>、允许牺牲一些性能和磁盘空间</td></tr></tbody></table><h3 id="混合持久化" tabindex="-1"><a class="header-anchor" href="#混合持久化"><span>混合持久化</span></a></h3><p>混合持久化是结合了 RDB 和 AOF 两种方式优点的一种持久化策略，能很好地兼顾重启速度和数据安全性。Redis 4.0 版本之后默认开启了混合持久化。</p><p><strong>工作原理</strong></p><ul><li>AOF 重写时，子进程先将当前数据以 <strong>RDB 格式</strong> 写入新 AOF 文件前半部分。</li><li>再将重写缓冲区的增量命令以 <strong>AOF 格式</strong> 追加到文件后半部分。</li><li>最终生成混合格式文件（<strong>RDB + AOF</strong>）。</li></ul><p><strong>优点</strong></p><ol><li><strong>重启效率高</strong>：先加载 RDB 部分快速恢复大部分数据，再重放增量命令。</li><li><strong>数据安全</strong>：结合 AOF 实时记录优势，保证了增量数据不丢失。</li><li>**文件体积可控：通过重写机制避免无限膨胀。</li></ol><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">#混合持久化开启配置，注意：此功能需要同时开启 AOF，即 appendonly yes</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">aof-use-rdb-preamble</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> yes</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="redis过期键删除策略" tabindex="-1"><a class="header-anchor" href="#redis过期键删除策略"><span>Redis过期键删除策略</span></a></h2><p>Redis 采用 <strong>惰性删除</strong> 和 <strong>定期删除</strong> 两种策略相结合的方式。</p><h3 id="惰性删除-lazy-expiration" tabindex="-1"><a class="header-anchor" href="#惰性删除-lazy-expiration"><span>惰性删除 (Lazy Expiration)</span></a></h3><ul><li><strong>做法</strong>：<strong>只有当客户端访问一个 key 时</strong>，Redis 才会检查这个 key 是否已过期。如果过期，则立即删除它，并且向客户端返回空值。</li><li><strong>优点</strong>：对 CPU 最友好。只会对访问的 key 进行过期检查，不会在无关的 key 上浪费 CPU 时间。</li><li><strong>缺点</strong>：对内存不友好。如果一个 key 已经过期，但永远不再被访问，那么它就永远不会被删除，会一直占用着内存，相当于<strong>内存泄漏</strong>。</li></ul><h3 id="定期删除-periodic-expiration" tabindex="-1"><a class="header-anchor" href="#定期删除-periodic-expiration"><span>定期删除 (Periodic Expiration)</span></a></h3><ul><li><strong>做法</strong>：Redis <strong>每隔一段时间</strong>（默认每秒 10 次，可配置）会随机抽取一批设置了过期时间的 key，检查它们是否过期。如果过期，则删除。 <ul><li><code>SLOW</code>模式：定时任务，默认10Hz（100ms一次），每次耗时<strong>不超过25ms</strong>。</li><li><code>FAST</code>模式：在事件循环中尝试执行，间隔≥2ms，耗时**≤1ms**，执行频率不固定</li></ul></li><li><strong>优点</strong>：通过限制执行的时长和频率，减少了对 CPU 的影响，同时也能<strong>定期清理</strong>掉过期的 key，弥补了惰性删除的不足。</li><li><strong>缺点</strong>：难以确定删除操作执行的时长和频率。太频繁会卡顿，太少了又和惰性删除一样。</li></ul><p><strong>小结</strong>：Redis 同时使用这两种策略，利用它们各自的优点，以达到在合理使用 CPU 时间和避免内存浪费之间取得平衡。</p><h2 id="redis内存淘汰策略" tabindex="-1"><a class="header-anchor" href="#redis内存淘汰策略"><span>Redis内存淘汰策略</span></a></h2><p>Redis内存淘汰策略(Memory Eviction Policy)，当 Redis 的内存使用达到 <code>maxmemory</code> 配置的上限时，会根据配置的淘汰策略来释放内存。</p><p>在 <code>redis.conf</code> 文件中通过 <code>maxmemory-policy</code> 配置项设置。常用的策略有：</p><table><thead><tr><th style="text-align:center;">策略</th><th>含义</th><th style="text-align:center;">淘汰范围</th><th style="text-align:center;">特点</th><th style="text-align:center;">适用场景</th></tr></thead><tbody><tr><td style="text-align:center;"><strong>noeviction</strong></td><td><strong>不淘汰</strong>。新命令返回错误。</td><td style="text-align:center;">不适用</td><td style="text-align:center;"><strong>默认策略</strong>。保证数据不丢失，但会牺牲写入可用性。</td><td style="text-align:center;">数据绝对不能丢失，将 Redis 作为<strong>数据库</strong>而非缓存的场景。</td></tr><tr><td style="text-align:center;"><strong>volatile-ttl</strong></td><td>优先淘汰<strong>剩余存活时间最短</strong>的键。</td><td style="text-align:center;">仅 <code>expire</code>键</td><td style="text-align:center;">根据 TTL 值决定，越快过期的越先被删除。</td><td style="text-align:center;">希望尽量保留那些还能存活很久的缓存数据。</td></tr><tr><td style="text-align:center;"><strong>volatile-random</strong></td><td><strong>随机</strong>淘汰一个设置了过期时间的键。</td><td style="text-align:center;">仅 <code>expire</code>键</td><td style="text-align:center;">实现简单，没有算法开销。</td><td style="text-align:center;">几乎不用。除非完全随机且不关心哪些数据被删除。</td></tr><tr><td style="text-align:center;"><strong>volatile-lru</strong></td><td>淘汰<strong>最近最少使用</strong>的键。</td><td style="text-align:center;">仅 <code>expire</code>键</td><td style="text-align:center;">基于访问时间，维护访问顺序。淘汰<strong>最久未访问</strong>的冷数据。</td><td style="text-align:center;"><strong>常用</strong>。明确区分“缓存”和“持久数据”，只淘汰缓存。</td></tr><tr><td style="text-align:center;"><strong>volatile-lfu</strong></td><td>淘汰<strong>访问频率最低</strong>的键。</td><td style="text-align:center;">仅 <code>expire</code>键</td><td style="text-align:center;">基于访问次数，统计访问频率。淘汰<strong>最不活跃</strong>的冷数据。</td><td style="text-align:center;">访问模式难以预测，希望淘汰那些长期“无人问津”的冷数据。</td></tr><tr><td style="text-align:center;"><strong>allkeys-random</strong></td><td><strong>随机</strong>淘汰任意一个键。</td><td style="text-align:center;">所有键</td><td style="text-align:center;">实现简单，无差别随机删除。</td><td style="text-align:center;">几乎不用。</td></tr><tr><td style="text-align:center;"><strong>allkeys-lru</strong></td><td>淘汰所有键中<strong>最近最少使用</strong>的键。</td><td style="text-align:center;">所有键</td><td style="text-align:center;"><strong>最常用、最通用</strong>的策略。基于访问时间淘汰冷数据。</td><td style="text-align:center;"><strong>生产环境首选</strong>。数据访问符合幂律分布（有热点数据），不确定用什么时选它。</td></tr><tr><td style="text-align:center;"><strong>allkeys-lfu</strong></td><td>淘汰所有键中<strong>访问频率最低</strong>的键。</td><td style="text-align:center;">所有键</td><td style="text-align:center;">基于访问频率，能更好区分热点和偶然访问的数据。</td><td style="text-align:center;">对冷热数据区分要求极高，有大量扫描式访问的场景。</td></tr></tbody></table><p><strong>建议</strong>：通常推荐使用 <strong><code>allkeys-lru</code></strong> 策略，因为它能很好地利用“二八定律”，将最常访问的数据保留在内存中。</p><blockquote><p>LRU(LeastRecentlyUsed)，最少最近使用。用当前时间减去最后一次访问时间，这个值越大则淘汰优先级越高。<br> LFU(Least Frequently Used)，最少频率使用。会统计每个key的访问频率，值越小淘汰优先级越高。</p></blockquote><hr><figure><img src="https://gitee.com/ishupei/picgo_img/raw/master/typora/image-20250906192554108.png" alt="image-20250906192554108" tabindex="0" loading="lazy"><figcaption>image-20250906192554108</figcaption></figure><h2 id="过期字典-expires-dictionary" tabindex="-1"><a class="header-anchor" href="#过期字典-expires-dictionary"><span>过期字典 (Expires Dictionary)</span></a></h2><p>Redis是如何知道一个key是否过期呢?Redis 通过其内部的 <strong>过期字典（expires dictionary）</strong> 来高效地管理和检查 key 的过期时间。（可以想象成一个哈希表）。</p><ul><li><strong>键 (Key)</strong>：指向真正数据库里那个 key 的指针。</li><li><strong>值 (Value)</strong>：一个 <code>long long</code> 类型的整数，保存了该 key 的<strong>绝对过期时间戳</strong>（以毫秒为单位）。</li></ul><p>例如，当你执行命令 <code>SET mykey &quot;hello&quot; EX 60</code>：</p><ol><li>Redis 会将 <code>mykey</code> 和它的值 <code>&quot;hello&quot;</code> 存入<strong>主数据库字典</strong>。</li><li>同时，它会计算一个 60 秒后的绝对时间戳（比如 <code>1646123456000</code>）。</li><li>然后将 <code>mykey</code> 和这个时间戳 <code>1646123456000</code> 作为一个键值对存入<strong>过期字典</strong>。</li></ol><hr><p><strong>检查过期的过程</strong></p><p>当需要判断一个 key 是否过期时（无论是在访问时还是定期删除时），Redis 只需要：</p><ol><li><strong>检查存在性</strong>：首先检查这个 key 是否存在于过期字典中。 <ul><li>如果不存在，说明该 key没有设置过期时间，永不过期。</li></ul></li><li><strong>比对时间戳</strong>：如果存在，则取出过期字典中存储的时间戳，与当前的系统时间戳进行比对。 <ul><li>如果 <strong>当前时间 &gt; 过期时间戳</strong>，则该 key<strong>已过期</strong>。</li><li>如果 <strong>当前时间 &lt;= 过期时间戳</strong>，则该 key<strong>未过期</strong>。</li></ul></li></ol><p>这个过程是一次高效的哈希查找和一次整数比较，速度极快。</p>`,86)]))}const d=s(l,[["render",r]]),g=JSON.parse(`{"path":"/database/redis/basic.html","title":"Redis基础","lang":"zh-CN","frontmatter":{"title":"Redis基础","category":"数据库","tag":["Redis基础"],"head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Redis基础\\",\\"image\\":[\\"https://gitee.com/ishupei/picgo_img/raw/master/typora/image-20250906192554108.png\\"],\\"dateModified\\":\\"2025-09-06T16:05:42.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"HuPei\\",\\"url\\":\\"https://ishupei.github.io\\"}]}"],["meta",{"property":"og:url","content":"https://ishupei.github.io/database/redis/basic.html"}],["meta",{"property":"og:site_name","content":"hupei's page"}],["meta",{"property":"og:title","content":"Redis基础"}],["meta",{"property":"og:description","content":"Redis数据类型 核心数据类型 (5种) String (字符串)： 最简单的类型，可存文本、整数或浮点数，二进制安全。支持自增/自减操作。 Hash (哈希)： 字段-值对的集合，非常适合存储对象。 List (列表)： 按插入顺序排序的字符串元素列表，基于双向链表实现，可用作栈或队列。 Set (集合)： 无序的、元素唯一的字符串集合，支持交集、..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://gitee.com/ishupei/picgo_img/raw/master/typora/image-20250906192554108.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-09-06T16:05:42.000Z"}],["meta",{"property":"article:tag","content":"Redis基础"}],["meta",{"property":"article:modified_time","content":"2025-09-06T16:05:42.000Z"}]],"description":"Redis数据类型 核心数据类型 (5种) String (字符串)： 最简单的类型，可存文本、整数或浮点数，二进制安全。支持自增/自减操作。 Hash (哈希)： 字段-值对的集合，非常适合存储对象。 List (列表)： 按插入顺序排序的字符串元素列表，基于双向链表实现，可用作栈或队列。 Set (集合)： 无序的、元素唯一的字符串集合，支持交集、..."},"git":{"createdTime":1757163672000,"updatedTime":1757174742000,"contributors":[{"name":"hupei","username":"hupei","email":"ishupei@qq.com","commits":2,"url":"https://github.com/hupei"}]},"readingTime":{"minutes":15.5,"words":4650},"filePathRelative":"database/redis/basic.md","autoDesc":true}`);export{d as comp,g as data};
